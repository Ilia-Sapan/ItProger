2.1.1. Supervised Learning
Supervised-Learning-Algorithmen lassen sich wiederum in zwei Kategorien
aufteilen:
• Regressionsanalyse
- Lineare Regression
- Multiple Regression
- Polynom-Regression
• Klassifizierung
- Logistische Regression
-  K-nearest-neighbors
-  Decision Trees
-  Random Forests
-  Naive Bayes
-  Support Vector Machines
- Neuronale Netze

2.1.2. Unsupervised Learning
• K-Means Clustering
• Agglomeratives Hierarchisches Clustering
• Divisive Hierarchical Clustering
• DBSCAN (Density based spatial clustering of applications with noise)

2.1.3.Reinforcement Learning

2.1.4. Trainings- und Testdaten

2.1.5. Over- und Underfitting

2.2. Lineare Regression

2.1.1. Обучение с учителем (Supervised Learning)
Это подход машинного обучения, при котором модель обучается на размеченных данных (имеется правильный ответ).

Два основных типа задач:

Регрессионный анализ (Regression) – прогнозирование числовых значений:

Линейная регрессия – зависимость одной переменной от другой описывается прямой линией.
Множественная регрессия – предсказание значения на основе нескольких переменных.
Полиномиальная регрессия – модель описывается полиномом (например, квадратичная зависимость).
Классификация (Classification) – отнесение данных к определённым классам:

Логистическая регрессия – вероятность принадлежности объекта к классу.
Метод k-ближайших соседей (KNN) – классификация на основе ближайших к объекту соседей.
Деревья решений (Decision Trees) – классификация с использованием логических правил в форме дерева.
Случайный лес (Random Forest) – ансамбль деревьев решений, повышающий точность и снижающий переобучение.
Наивный Байес (Naive Bayes) – вероятностный классификатор на основе теоремы Байеса.
Метод опорных векторов (SVM) – поиск оптимальной разделяющей границы между классами.
Нейронные сети – сложные нелинейные модели, вдохновлённые работой человеческого мозга.
2.1.2. Обучение без учителя (Unsupervised Learning)
Используется на немаркированных данных для выявления скрытых структур.

Методы кластеризации (выделения групп объектов):

K-Means: Разделяешь объекты на несколько групп (кластеров) так, чтобы внутри каждой группы объекты были похожи друг на друга.
Иерархическая кластеризация (агломеративная): Сначала каждый объект отдельно, затем ты постепенно объединяешь похожие объекты и группы между собой.
Иерархическая кластеризация (дивизивная): Сначала все объекты в одной группе, потом постепенно делишь её на меньшие группы.
DBSCAN: Ищешь группы, в которых объекты находятся близко друг к другу. Этот метод умеет хорошо отсеивать отдельные шумные данные.
2.1.3. Обучение с подкреплением (Reinforcement Learning)
Модель (агент) учится принимать решения, получая вознаграждение или штраф от среды. Используется, например, в играх и робототехнике.

2.1.4. Обучающие и тестовые данные
Данные делятся на две части:

Train-дата (обучающие) – для обучения модели.
Test-дата (тестовые) – для оценки качества модели на ранее не виденных данных.
2.1.5. Переобучение (Overfitting) и недообучение (Underfitting)
Overfitting – модель слишком хорошо обучилась на тренировочных данных, но плохо обобщает новые данные.
Underfitting – модель плохо обучилась, недостаточно хорошо описывает данные.
Цель – сбалансированная модель, хорошо работающая на новых данных.

2.2. Линейная регрессия
Модель, описывающая связь между зависимой и независимой переменными с помощью прямой линии.
Используется для предсказания непрерывных числовых значений.